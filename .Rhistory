html_nodes(".a-icon-alt") %>%
html_text()
product_text_review <- webpage %>%
html_nodes(".review-text-content span") %>%
html_text()
product_title_of_review <- webpage %>%
html_nodes(".review-title span") %>%
html_text()
product_verified_purchase <- webpage %>%
html_nodes(".review-format-strip") %>%
html_text()
# Synchronize lengths
min_length <- min(
length(product_reviewers),
length(product_ratings),
length(product_date),
length(product_text_review),
length(product_title_of_review),
length(product_verified_purchase)
)
if (min_length > 0) {
product_reviewers <- product_reviewers[1:min_length]
product_ratings <- product_ratings[1:min_length]
product_date <- product_date[1:min_length]
product_text_review <- product_text_review[1:min_length]
product_title_of_review <- product_title_of_review[1:min_length]
product_verified_purchase <- product_verified_purchase[1:min_length]
# Store data in a dataframe
df <- data.frame(
Reviewers = product_reviewers,
Ratings = product_ratings,
Date = product_date,
Individual_Rating = product_individual_rating[1:min_length],
Text_Review = product_text_review,
Title_of_Review = product_title_of_review,
Verified_Purchase = ifelse(grepl("Verified Purchase", product_verified_purchase), "Yes", "No"),
stringsAsFactors = FALSE
)
# Append dataframe to the list
all_data[[i]] <- df
} else {
cat("No data found for URL:", urls[i], "\n")
}
}
# Combine all dataframes into one
final_df <- bind_rows(all_data)
# View the data
if (nrow(final_df) > 0) {
View(final_df)
print(head(final_df))
} else {
cat("No data was collected from any of the URLs.\n")
}
library(rvest)
library(httr)
library(polite)
library(dplyr)
library(stringr)
urls <- c(
"https://www.amazon.com/s/ref=nb_sb_noss?url=search-alias%3Dcomputers-intl-ship&field-keywords=&crid=26NA7NBV28LKS&sprefix=%2Ccomputers-intl-ship%2C294",
"https://www.amazon.com/s/ref=nb_sb_noss?url=search-alias%3Dautomotive-intl-ship&field-keywords=&crid=U226PZ65YJDJ&sprefix=%2Cautomotive-intl-ship%2C1487",
"https://www.amazon.com/s/ref=nb_sb_noss?url=search-alias%3Dbaby-products-intl-ship&field-keywords=&crid=353R0P6ALFQDF&sprefix=%2Cbaby-products-intl-ship%2C382",
"https://www.amazon.com/s/ref=nb_sb_noss?url=search-alias%3Delectronics-intl-ship&field-keywords=&crid=35DTFE12794DD&sprefix=%2Celectronics-intl-ship%2C302",
"https://www.amazon.com/s/ref=nb_sb_noss?url=search-alias%3Dsoftware-intl-ship&field-keywords=&crid=2F0IMEKT23811&sprefix=%2Csoftware-intl-ship%2C282"
)
all_data <- list()
for (i in seq_along(urls)) {
# Initialize polite session
session <- bow(urls[i], user_agent = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36")
# Fetch and parse the webpage
webpage <- try(read_html(session), silent = TRUE)
if (inherits(webpage, "try-error")) {
cat("Failed to fetch data for URL:", urls[i], "\n")
next
}
# Extract product data
product_ratings <- webpage %>%
html_nodes(".a-icon-alt") %>%
html_text()
product_reviewers <- webpage %>%
html_nodes(".a-profile-name") %>%
html_text()
product_date <- webpage %>%
html_nodes(".review-date") %>%
html_text()
product_individual_rating <- webpage %>%
html_nodes(".a-icon-alt") %>%
html_text()
product_text_review <- webpage %>%
html_nodes(".review-text-content span") %>%
html_text()
product_title_of_review <- webpage %>%
html_nodes(".review-title span") %>%
html_text()
product_verified_purchase <- webpage %>%
html_nodes(".review-format-strip") %>%
html_text()
# Synchronize lengths
min_length <- min(
length(product_reviewers),
length(product_ratings),
length(product_date),
length(product_text_review),
length(product_title_of_review),
length(product_verified_purchase)
)
if (min_length > 0) {
product_reviewers <- product_reviewers[1:min_length]
product_ratings <- product_ratings[1:min_length]
product_date <- product_date[1:min_length]
product_text_review <- product_text_review[1:min_length]
product_title_of_review <- product_title_of_review[1:min_length]
product_verified_purchase <- product_verified_purchase[1:min_length]
# Store data in a dataframe
df <- data.frame(
Reviewers = product_reviewers,
Ratings = product_ratings,
Date = product_date,
Individual_Rating = product_individual_rating[1:min_length],
Text_Review = product_text_review,
Title_of_Review = product_title_of_review,
Verified_Purchase = ifelse(grepl("Verified Purchase", product_verified_purchase), "Yes", "No"),
stringsAsFactors = FALSE
)
# Append dataframe to the list
all_data[[i]] <- df
} else {
cat("No data found for URL:", urls[i], "\n")
}
}
# Combine all dataframes into one
final_df <- bind_rows(all_data)
# View the data
if (nrow(final_df) > 0) {
View(final_df)
print(head(final_df))
} else {
cat("No data was collected from any of the URLs.\n")
}
library(rvest)
library(httr)
library(polite)
library(dplyr)
library(stringr)
urls <- c(
"https://www.amazon.com/s/ref=nb_sb_noss?url=search-alias%3Dcomputers-intl-ship&field-keywords=&crid=26NA7NBV28LKS&sprefix=%2Ccomputers-intl-ship%2C294",
"https://www.amazon.com/s/ref=nb_sb_noss?url=search-alias%3Dautomotive-intl-ship&field-keywords=&crid=U226PZ65YJDJ&sprefix=%2Cautomotive-intl-ship%2C1487",
"https://www.amazon.com/s/ref=nb_sb_noss?url=search-alias%3Dbaby-products-intl-ship&field-keywords=&crid=353R0P6ALFQDF&sprefix=%2Cbaby-products-intl-ship%2C382",
"https://www.amazon.com/s/ref=nb_sb_noss?url=search-alias%3Delectronics-intl-ship&field-keywords=&crid=35DTFE12794DD&sprefix=%2Celectronics-intl-ship%2C302",
"https://www.amazon.com/s/ref=nb_sb_noss?url=search-alias%3Dsoftware-intl-ship&field-keywords=&crid=2F0IMEKT23811&sprefix=%2Csoftware-intl-ship%2C282"
)
all_data <- list()
for (i in seq_along(urls)) {
# Initialize polite session
session <- bow(urls[i], user_agent = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36")
# Fetch and parse the webpage
webpage <- try(read_html(session), silent = TRUE)
if (inherits(webpage, "try-error")) {
cat("Failed to fetch data for URL:", urls[i], "\n")
next
}
# Extract product data
product_ratings <- webpage %>%
html_nodes(".a-icon-alt") %>%
html_text()
product_reviewers <- webpage %>%
html_nodes(".a-profile-name") %>%
html_text()
product_date <- webpage %>%
html_nodes(".review-date") %>%
html_text()
product_individual_rating <- webpage %>%
html_nodes(".a-icon-alt") %>%
html_text()
product_text_review <- webpage %>%
html_nodes(".review-text-content span") %>%
html_text()
product_title_of_review <- webpage %>%
html_nodes(".review-title span") %>%
html_text()
product_verified_purchase <- webpage %>%
html_nodes(".review-format-strip") %>%
html_text()
# Synchronize lengths
min_length <- min(
length(product_reviewers),
length(product_ratings),
length(product_date),
length(product_text_review),
length(product_title_of_review),
length(product_verified_purchase)
)
if (min_length > 0) {
product_reviewers <- product_reviewers[1:min_length]
product_ratings <- product_ratings[1:min_length]
product_date <- product_date[1:min_length]
product_text_review <- product_text_review[1:min_length]
product_title_of_review <- product_title_of_review[1:min_length]
product_verified_purchase <- product_verified_purchase[1:min_length]
# Store data in a dataframe
df <- data.frame(
Reviewers = product_reviewers,
Ratings = product_ratings,
Date = product_date,
Individual_Rating = product_individual_rating[1:min_length],
Text_Review = product_text_review,
Title_of_Review = product_title_of_review,
Verified_Purchase = ifelse(grepl("Verified Purchase", product_verified_purchase), "Yes", "No"),
stringsAsFactors = FALSE
)
# Append dataframe to the list
all_data[[i]] <- df
} else {
cat("No data found for URL:", urls[i], "\n")
}
}
# Combine all dataframes into one
final_df <- bind_rows(all_data)
# View the data
if (nrow(final_df) > 0) {
View(final_df)
print(head(final_df))
} else {
cat("No data was collected from any of the URLs.\n")
}
library(rvest)
library(httr)
library(dplyr)
library(polite)
library(stringr)
urls <- c(
'https://www.amazon.com/s?k=backpacks&crid=35ZQ1H72MC3G9&sprefix=backpacks%2Caps%2C590&ref=nb_sb_ss_ts-doa-p_3_9',
'https://www.amazon.com/s?k=laptops&crid=L7MQBW7MD4SX&sprefix=laptopb%2Caps%2C1304&ref=nb_sb_noss_2',
'https://www.amazon.com/s?k=phone+case&dc&crid=1VPDCJ87S93TL&sprefix=phone+cas%2Caps%2C451&ref=a9_asc_1',
'https://www.amazon.com/s?k=mountain+bike&crid=1ZQR71S8XHZN6&sprefix=mountain+bik%2Caps%2C499&ref=nb_sb_noss_2',
'https://www.amazon.com/s?k=tshirt&crid=2RQIP7MP6IYAW&sprefix=tshirt%2Caps%2C443&ref=nb_sb_noss_2'
)
df <- list()
reviews <- list()
for (i in seq_along(urls)) {
session <- bow(urls[i], user_agent = "Educational")
product_name <- scrape(session) %>%
html_nodes('h2.a-size-mini') %>%
html_text() %>%
head(30)
product_description <- scrape(session) %>%
html_nodes('div.productDescription') %>%
html_text() %>%
head(30)
product_rating <- scrape(session) %>%
html_nodes('span.a-icon-alt') %>%
html_text() %>%
head(30)
ratings <- as.numeric(str_extract(product_rating, "\\d+\\.\\d"))
product_price <- scrape(session) %>%
html_nodes('span.a-price') %>%
html_text() %>%
head(30)
price <- as.numeric(str_extract(product_price, "\\d+\\.\\d+"))
dfTemp <- data.frame(Product_Name = product_name[1:30],
Description = product_description[1:30],
Rating = ratings[1:30],
Price = price[1:30],
stringsAsFactors = FALSE)
df[[i]] <- dfTemp
selectTen_products <- head(product_name, 10)
for (j in seq_along(selectTen_products)) {
reviewers <- scrape(session) %>%
html_nodes('div.a-profile-content') %>%
html_text() %>%
head(20)
review_dates <- scrape(session) %>%
html_nodes('span.review-date') %>%
html_text() %>%
head(20)
date_only <- str_extract(review_dates, "[A-Za-z]+ \\d{1,2}, \\d{4}")
review_dates_parsed <- as.Date(date_only, format = "%B %d, %Y")
review_ratings <- scrape(session) %>%
html_nodes('i.review-rating') %>%
html_text() %>%
head(20)
individual_ratings <- as.numeric(str_extract(review_ratings, "\\d+\\.\\d"))
review_titles <- scrape(session) %>%
html_nodes('span.a-letter-space') %>%
html_text() %>%
head(20)
review_texts <- scrape(session) %>%
html_nodes('span.review-text') %>%
html_text() %>%
head(20)
verified_status <- scrape(session) %>%
html_nodes('span.a-class-mini') %>%
html_text() %>%
head(20)
reviewsTemp <- data.frame(
Reviewer = reviewers[1:20],
Date = review_dates_parsed[1:20],
Rating = individual_ratings[1:20],
Title = review_titles[1:20],
Review = review_texts[1:20],
Verified = verified_status[1:20],
stringsAsFactors = FALSE
)
reviews[[j]] <- reviewsTemp
}
}
print(df[[1]])
print(df[[2]])
print(df[[3]])
print(df[[4]])
print(df[[5]])
print(reviews[[1]])
print(reviews[[2]])
print(reviews[[3]])
print(reviews[[4]])
print(reviews[[5]])
print(reviews[[6]])
print(reviews[[7]])
print(reviews[[8]])
print(reviews[[9]])
print(reviews[[10]])
knitr::opts_chunk$set(echo = TRUE)
#Load data
data = tweets_df.parse('tweetsDF');
#Load data
data = tweets_df.parse('tweetsDF')
#Load data
data = tweets_df.parse('tweetsDF')
#Load data
data = tweetsDF('tweetsDF')
str(tweetsDF)
View(tweetsDF)
```{r}
knitr::opts_chunk$set(echo = TRUE)
library(readxl)
tweetsDF <- read_excel("C:/PROJ/tweetsDF.xlsx")
View(tweetsDF)
tweets_clean <- tweetsDF[is.na(tweets_df$text), ]
tweets_clean <- tweetsDF[is.na(tweets_DF$text), ]
```{r}
library(readxl)
tweetsDF <- read_excel("C:/PROJ/tweetsDF.xlsx")
View(tweetsDF)
str(tweetsDF)
colSums(is.na(twwetsDF))
library(readxl)
tweetsDF <- read_excel("C:/PROJ/tweetsDF.xlsx")
View(tweetsDF)
str(tweetsDF)
colSums(is.na(tweetsDF))
tweets_clean <- tweetsDF[is.na(tweets_DF$text), ]
library(readxl)
tweetsDF <- read_excel("C:/PROJ/tweetsDF.xlsx")
View(tweetsDF)
str(tweetsDF)
colSums(is.na(tweetsDF))
tweets_clean <- tweetsDF[is.na(tweetsDF$text), ]
tweets_clean <- tweetsDF[is.na(tweetsDF$text), ]
library(readxl)
tweetsDF <- read_excel("C:/PROJ/tweetsDF.xlsx")
View(tweetsDF)
str(tweetsDF)
colSums(is.na(tweetsDF))
tweets_clean <- tweetsDF[is.na(tweetsDF$text), ]
tweets_clean$created <- as.POSIXct(tweets_clean$created, format = "%Y-%m-%d %H:%M:%S") #convert to date and time
library(readxl)
tweetsDF <- read_excel("C:/PROJ/tweetsDF.xlsx")
View(tweetsDF)
str(tweetsDF)
colSums(is.na(tweetsDF))
tweets_clean <- tweetsDF[is.na(tweetsDF$text), ]
tweets_clean$created <- as.POSIXct(tweets_clean$created, format = "%Y-%m-%d %H:%M:%S") #convert to date and time
install.packages("syuzhet")
library(syuzhet)
knitr::opts_chunk$set(echo = TRUE)
library(readxl)
tweetsDF <- read_excel("C:/PROJ/tweetsDF.xlsx")
View(tweetsDF)
str(tweetsDF)
colSums(is.na(tweetsDF))
tweets_clean <- tweetsDF[is.na(tweetsDF$text), ]
tweets_clean$created <- as.POSIXct(tweets_clean$created, format = "%Y-%m-%d %H:%M:%S") #convert to date and time
install.packages("syuzhet")
library(syuzhet)
tweets_clean$sentiment <- get_sentiment(tweets_clean$text, method = "afinn")
knitr::opts_chunk$set(echo = TRUE)
library(readxl)
tweetsDF <- read_excel("C:/PROJ/tweetsDF.xlsx")
View(tweetsDF)
knitr::opts_chunk$set(echo = TRUE)
library(readxl)
library(dplyr)
library(ggplot2)
library(tidyr)
library(stringr)
library(syuzhet)
tweetsDF <- read_excel("C:/PROJ/tweetsDF.xlsx")
tweetsDF <- tweetsDF %>%
rename(
screen_name = screenName,
tweet = text,
created_at = created,
source = statusSource,
rounded_time = Created_At_Round,
tweet_source = tweetSource
)
tweets_sentiment <- tweetsDF %>%
mutate(sentiment_score = get_sentiment(tweet, method = "bing")) %>%
group_by(date = as.Date(created_at)) %>%
summarise(
avg_sentiment = mean(sentiment_score),
positive_tweets = sum(sentiment_score > 0),
negative_tweets = sum(sentiment_score < 0)
)
knitr::opts_chunk$set(echo = TRUE)
library(readxl)
library(dplyr)
library(ggplot2)
library(tidyr)
library(stringr)
library(syuzhet)
tweetsDF <- read_excel("C:/PROJ/tweetsDF.xlsx")
tweetsDF <- tweetsDF %>%
rename(
screen_name = screenName,
tweet = text,
created_at = created,
source = statusSource,
rounded_time = Created_At_Round,
tweet_source = tweetSource
)
tweets_sentiment <- tweetsDF %>%
mutate(sentiment_score = get_sentiment(tweet, method = "bing")) %>%
group_by(date = as.Date(created_at)) %>%
summarise(
avg_sentiment = mean(sentiment_score),
positive_tweets = sum(sentiment_score > 0),
negative_tweets = sum(sentiment_score < 0)
)
library(readxl)
library(dplyr)
library(ggplot2)
library(tidyr)
library(stringr)
library(syuzhet)
tweetsDF <- read_excel("C:/PROJ/tweetsDF.xlsx")
tweetsDF <- tweetsDF %>%
rename(
screen_name = screenName,
tweet = text,
created_at = created,
source = statusSource,
rounded_time = Created_At_Round,
tweet_source = tweetSource
)
tweets_sentiment <- tweetsDF %>%
mutate(sentiment_score = get_sentiment(tweet, method = "bing")) %>%
group_by(date = as.Date(created_at)) %>%
summarise(
avg_sentiment = mean(sentiment_score),
positive_tweets = sum(sentiment_score > 0),
negative_tweets = sum(sentiment_score < 0)
)
# plot sentiment analysis results
sentiment_plot <- ggplot(tweets_sentiment, aes(x = date)) +
geom_line(aes(y = avg_sentiment, color = "Average Sentiment"), linewidth = 1) +
geom_line(aes(y = positive_tweets, color = "Positive Tweets"), linewidth = 1) +
geom_line(aes(y = negative_tweets, color = "Negative Tweets"), linewidth = 1) +
labs(
title = "Sentiment Analysis of Tweets Over Time",
x = "Date",
y = "Sentiment/Count",
color = "Metrics"
) +
theme_minimal()
library(readxl)
library(dplyr)
library(ggplot2)
library(tidyr)
library(stringr)
library(syuzhet)
tweetsDF <- read_excel("C:/PROJ/tweetsDF.xlsx")
tweetsDF <- tweetsDF %>%
rename(
screen_name = screenName,
tweet = text,
created_at = created,
source = statusSource,
rounded_time = Created_At_Round,
tweet_source = tweetSource
)
tweets_sentiment <- tweetsDF %>%
mutate(sentiment_score = get_sentiment(tweet, method = "bing")) %>%
group_by(date = as.Date(created_at)) %>%
summarise(
avg_sentiment = mean(sentiment_score),
positive_tweets = sum(sentiment_score > 0),
negative_tweets = sum(sentiment_score < 0)
)
# plot sentiment analysis results
sentiment_plot <- ggplot(tweets_sentiment, aes(x = date)) +
geom_line(aes(y = avg_sentiment, color = "Average Sentiment"), linewidth = 1) +
geom_line(aes(y = positive_tweets, color = "Positive Tweets"), linewidth = 1) +
geom_line(aes(y = negative_tweets, color = "Negative Tweets"), linewidth = 1) +
labs(
title = "Sentiment Analysis of Tweets Over Time",
x = "Date",
y = "Sentiment/Count",
color = "Metrics"
) +
theme_minimal()
print(sentiment_plot)
